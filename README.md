# Databricks Data Engineering Project

## Project Goal

The primary goal of this project is to gain a deeper understanding of **Azure Databricks** and explore its powerful suite of data engineering tools and services. This project demonstrates how to build an **end-to-end data pipeline** using **Databricks Workflows**, **Delta Live Tables**, and **Unity Catalog**.

---

## Key Technologies & Concepts

- **Azure Databricks**
- **Databricks Workflows & Dataflows**
- **Unity Catalog** for data governance and schema management
- **Delta Live Tables** for streaming and batch transformations
- **Slowly Changing Dimensions (SCD) Type 1 & Type 2**

---

## Project Workflow

This project includes the creation and management of complete **data engineering pipelines** within Databricks, showcasing how to:

- Design and schedule pipelines using **Databricks Workflows**
- Build **Delta Live Tables (DLTs)** to process and transform data
- Implement **Slowly Changing Dimensions** (Type 1 and Type 2) to handle real-world business changes in datasets
- Leverage **Unity Catalog** for improved data lineage, access control, and discoverability

---

## Architecture Overview

Below is a high-level snapshot of the pipeline built in Databricks:

![Final Pipeline](https://github.com/user-attachments/assets/0e574376-9d5f-4c9f-bba3-b13707734735)

---

##  Summary

This project showcases the capabilities of **Databricks as a unified data engineering platform**. By combining features like **DLTs**, **Unity Catalog**, and advanced SCD handling, it provides a scalable and production-ready framework for managing complex data workflows in real-time or batch mode.

---

